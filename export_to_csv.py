#!/usr/bin/env python3
"""Export JSON outputs to CSV format.

Usage:
    python export_to_csv.py                           # export all categories
    python export_to_csv.py --category paper1         # export specific category
    python export_to_csv.py --merged                  # export from merged JSON files
    python export_to_csv.py --output results.csv      # custom output filename

This script reads JSON files generated by the PDF processor and exports them
to CSV format for easy analysis in Excel, Google Sheets, or data analysis tools.
"""

import argparse
import csv
import json
import sys
from pathlib import Path
from typing import List, Dict, Any


def flatten_json(data: Dict[Any, Any], parent_key: str = "", sep: str = "_") -> Dict[str, Any]:
    """Flatten nested JSON structure into a single-level dictionary."""
    items = []
    for key, value in data.items():
        new_key = f"{parent_key}{sep}{key}" if parent_key else key
        
        if isinstance(value, dict):
            items.extend(flatten_json(value, new_key, sep=sep).items())
        elif isinstance(value, list):
            # Convert lists to string representation
            if value and isinstance(value[0], dict):
                # List of dicts - serialize as JSON
                items.append((new_key, json.dumps(value, ensure_ascii=False)))
            else:
                # List of primitives - join with semicolon
                items.append((new_key, "; ".join(str(v) for v in value)))
        else:
            items.append((new_key, value))
    
    return dict(items)


def collect_json_files(category_dir: Path, use_merged: bool = False) -> List[Path]:
    """Collect JSON files from category directory."""
    if use_merged:
        merged_file = category_dir / f"{category_dir.name}_merged.json"
        return [merged_file] if merged_file.exists() else []
    
    files = [
        path
        for path in category_dir.glob("*.json")
        if path.is_file() and not path.name.endswith("_merged.json")
    ]
    return sorted(files, key=lambda p: p.name.lower())


def load_json_data(json_files: List[Path], use_merged: bool = False) -> List[Dict[str, Any]]:
    """Load and flatten JSON data from files."""
    all_data = []
    
    for json_file in json_files:
        try:
            with json_file.open("r", encoding="utf-8") as f:
                content = json.load(f)
                
                # If merged file, it's a list of papers
                if use_merged and isinstance(content, list):
                    for item in content:
                        all_data.append(flatten_json(item))
                else:
                    all_data.append(flatten_json(content))
                    
        except json.JSONDecodeError as e:
            print(f"âš ï¸  Warning: Failed to parse {json_file}: {e}")
            continue
        except Exception as e:
            print(f"âš ï¸  Warning: Error reading {json_file}: {e}")
            continue
    
    return all_data


def export_category_to_csv(category_dir: Path, output_file: Path, use_merged: bool = False) -> int:
    """Export a single category to CSV."""
    json_files = collect_json_files(category_dir, use_merged)
    
    if not json_files:
        print(f"âš ï¸  No JSON files found in {category_dir}")
        return 0
    
    # Load all data
    all_data = load_json_data(json_files, use_merged)
    
    if not all_data:
        print(f"âš ï¸  No valid data found in {category_dir}")
        return 0
    
    # Get all unique keys across all papers
    all_keys = set()
    for item in all_data:
        all_keys.update(item.keys())
    
    # Sort keys for consistent column order
    fieldnames = sorted(all_keys)
    
    # Write CSV
    with output_file.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        writer.writeheader()
        writer.writerows(all_data)
    
    return len(all_data)


def export_all_categories(root: Path, output_file: Path, use_merged: bool = False) -> int:
    """Export all categories to a single CSV file."""
    if not root.exists() or not root.is_dir():
        print(f"âŒ Root directory not found: {root}")
        return 1
    
    category_dirs = [d for d in sorted(root.iterdir()) if d.is_dir()]
    if not category_dirs:
        print(f"âŒ No category directories found under {root}")
        return 1
    
    all_data = []
    total_papers = 0
    
    for category_dir in category_dirs:
        json_files = collect_json_files(category_dir, use_merged)
        if not json_files:
            continue
        
        data = load_json_data(json_files, use_merged)
        all_data.extend(data)
        total_papers += len(data)
        print(f"âœ… Loaded {len(data)} papers from {category_dir.name}")
    
    if not all_data:
        print("âŒ No data to export")
        return 1
    
    # Get all unique keys
    all_keys = set()
    for item in all_data:
        all_keys.update(item.keys())
    
    fieldnames = sorted(all_keys)
    
    # Write CSV
    with output_file.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        writer.writeheader()
        writer.writerows(all_data)
    
    print(f"\nâœ… Exported {total_papers} papers to {output_file}")
    print(f"ðŸ“Š Total columns: {len(fieldnames)}")
    
    return 0


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--category",
        type=str,
        help="Export specific category only",
    )
    parser.add_argument(
        "--root",
        type=Path,
        default=Path("data/output"),
        help="Root directory containing category subdirectories (default: data/output)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("papers_export.csv"),
        help="Output CSV filename (default: papers_export.csv)",
    )
    parser.add_argument(
        "--merged",
        action="store_true",
        help="Use merged JSON files (*_merged.json) instead of individual files",
    )
    return parser


def main(argv: List[str]) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    
    if args.category:
        # Export specific category
        category_dir = args.root / args.category
        if not category_dir.exists():
            print(f"âŒ Category directory not found: {category_dir}")
            return 1
        
        output_file = args.output
        if output_file.name == "papers_export.csv":
            # Use category-specific name
            output_file = Path(f"{args.category}_export.csv")
        
        count = export_category_to_csv(category_dir, output_file, args.merged)
        if count > 0:
            print(f"âœ… Exported {count} papers from {args.category} to {output_file}")
            return 0
        else:
            return 1
    else:
        # Export all categories
        return export_all_categories(args.root, args.output, args.merged)


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
